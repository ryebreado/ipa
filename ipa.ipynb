{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be269a92-d595-4c92-a525-d150faae1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/ipa-dict-master/data/en_US.txt\", sep=\"\\t\", header=None, keep_default_na=False)\n",
    "df.columns = [\"word_original\", \"ipa_original\"]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e282705-461f-4187-84bd-7b581b111035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df[\"word\"] = df[\"word_original\"].apply(str.lower)\n",
    "\n",
    "def normalize_ipa(ipa_original):\n",
    "    result = re.search(r\"/[^/]+/$\", ipa_original)\n",
    "    if result:\n",
    "        return result.group()[1:-1]\n",
    "    return ipa_original\n",
    "\n",
    "df[\"ipa\"] = df[\"ipa_original\"].apply(normalize_ipa)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be51ae-3eb7-4f88-b2cd-c7ce6736f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class Direction(Enum):\n",
    "    TO_IPA = 1\n",
    "    FROM_IPA = 2\n",
    "    \n",
    "class Sublanguage(Enum):\n",
    "    WORD = 1\n",
    "    IPA = 2\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Alphabet:\n",
    "    locale: str\n",
    "    sublanguage: Sublanguage\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.locale}:{self.sublanguage.name}\"\n",
    "\n",
    "LOCALE = \"en_US\"\n",
    "DIRECTION = Direction.TO_IPA\n",
    "\n",
    "SRC_SUBLANGUAGE = Sublanguage.WORD if DIRECTION == Direction.TO_IPA else Sublanguage.IPA\n",
    "TGT_SUBLANGUAGE = Sublanguage.IPA if DIRECTION == Direction.TO_IPA else Sublanguage.WORD\n",
    "\n",
    "SRC_LANGUAGE = Alphabet(LOCALE, SRC_SUBLANGUAGE)\n",
    "TGT_LANGUAGE = Alphabet(LOCALE, TGT_SUBLANGUAGE)\n",
    "\n",
    "print(f\"The model will translate {SRC_LANGUAGE} to {TGT_LANGUAGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b930e-69bd-4924-a2d4-7b79b3876e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(filename):\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", header=None, keep_default_na=False)\n",
    "    df.columns = [\"word_original\", \"ipa_original\"]\n",
    "    df[Sublanguage.WORD.name] = df[\"word_original\"].apply(str.lower)\n",
    "    df[Sublanguage.IPA.name] = df[\"ipa_original\"].apply(normalize_ipa)\n",
    "    return df\n",
    "\n",
    "df_en = create_dataframe(\"data/ipa-dict-master/data/en_US.txt\")\n",
    "display(df_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa520d6-fe4d-47f1-a65a-6cebde548d44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IpaDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, locale: str, direction: Direction, label: str | None = None):\n",
    "        self._df = df\n",
    "        self._direction = direction\n",
    "        self._locale = locale\n",
    "        self._label = label\n",
    "    \n",
    "    def direction(self) -> Direction:\n",
    "        return self._direction\n",
    "    \n",
    "    def locale(self) -> str:\n",
    "        return self._locale\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._df)\n",
    "    \n",
    "    def __getitem__(self, idx) -> list[str]:\n",
    "        row = self._df.iloc[idx]\n",
    "        if self._direction == Direction.TO_IPA:\n",
    "            return [row[Sublanguage.WORD.name], row[Sublanguage.IPA.name]]\n",
    "        else:\n",
    "            return [row[Sublanguage.IPA.name], row[Sublanguage.WORD.name]]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        opt_label = f\"[{self._label}]\" if self._label else \"\"\n",
    "        return f\"IpaDataset{opt_label}(size={len(self)}, locale={self.locale()}, dir={self.direction().name})\"\n",
    "\n",
    "ipa_en = IpaDataset(df_en, \"en_US\", Direction.TO_IPA, \"b\")\n",
    "print(ipa_en)\n",
    "print(len(ipa_en))\n",
    "print(ipa_en[400])\n",
    "print(ipa_en[400:402])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc8b5ae-c5cd-4a4e-85da-bbf1211fea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplit(Enum):\n",
    "    TRAIN = 1\n",
    "    VALIDATION = 2\n",
    "\n",
    "def tokenize(word: str) -> list[str]:\n",
    "    return list(word)\n",
    "\n",
    "class IpaDatasetHolder:\n",
    "    def __init__(self, \n",
    "                 locale: str, \n",
    "                 everything: pd.DataFrame,\n",
    "                 splits: dict[DataSplit, dict[Direction, IpaDataset]],\n",
    "                ):\n",
    "        self._locale = locale\n",
    "        self._everything = everything\n",
    "        self._splits = splits\n",
    "\n",
    "    def __getitem__(self, idx: tuple[DataSplit, Direction]) -> IpaDataset:\n",
    "        split, direction = idx\n",
    "        return self._splits[split][direction]\n",
    "\n",
    "    @classmethod\n",
    "    def create_holder_from_csv(cls, locale: str):\n",
    "        csv_path = f\"data/ipa-dict-master/data/{locale}.txt\"\n",
    "        df = create_dataframe(csv_path)\n",
    "        \n",
    "        df_train = df.sample(frac = 0.7)\n",
    "        df_validation = df.drop(df_train.index)\n",
    "        split_dfs = {DataSplit.TRAIN: df_train, DataSplit.VALIDATION: df_validation}\n",
    "        \n",
    "        splits = {}\n",
    "        for split, split_df in split_dfs.items():\n",
    "            splits[split] = {d:IpaDataset(split_df, locale, d, split.name) for d in Direction}\n",
    "\n",
    "        return cls(locale=locale, everything=df, splits=splits)\n",
    "    \n",
    "    def build_token_lists(self) -> dict[Sublanguage, list[str]]:\n",
    "        word_tokens = set()\n",
    "        ipa_tokens = set()\n",
    "        for _, row in self._everything.iterrows():\n",
    "            word_tokens.update(tokenize(row[Sublanguage.WORD.name]))\n",
    "            ipa_tokens.update(tokenize(row[Sublanguage.IPA.name]))\n",
    "        return {Sublanguage.WORD: sorted(word_tokens), Sublanguage.IPA: sorted(ipa_tokens)}\n",
    "                              \n",
    "\n",
    "en_holder = IpaDatasetHolder.create_holder_from_csv(\"en_US\")\n",
    "print(en_holder[DataSplit.TRAIN, Direction.TO_IPA])\n",
    "print(en_holder[DataSplit.VALIDATION, Direction.TO_IPA])\n",
    "token_lists_temp = en_holder.build_token_lists()\n",
    "print(token_lists_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534c71b-e76d-479e-91b4-c8ae0409bc89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d65b8-ba34-47ec-a83d-ae292420fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "\n",
    "# unknown, padding, beginning of string, end of string\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "def build_vocabs(locale: str) -> dict[Alphabet, Vocab]:\n",
    "    vocabs = {}\n",
    "    holder = IpaDatasetHolder.create_holder_from_csv(locale)\n",
    "    token_lists = holder.build_token_lists()\n",
    "    print(f\"{type(token_lists)=}\")\n",
    "    for sublanguage, token_list in token_lists.items():\n",
    "        alphabet = Alphabet(locale, sublanguage)\n",
    "        vocab = build_vocab_from_iterator(\n",
    "            token_list, min_freq = 1, specials = special_symbols, special_first = True)\n",
    "        vocab.set_default_index(UNK_IDX)\n",
    "        vocabs[alphabet] = vocab\n",
    "    return vocabs\n",
    "\n",
    "vocabs_english = build_vocabs(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efdc7a5-6706-4415-b9a4-6155b89a9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://pytorch.org/tutorials/beginner/translation_transformer.html#seq2seq-network-using-transformer\n",
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                tgt: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d323cd6-85c4-453d-91ef-25c1663e685b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
